{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShannonH98/PyTorch_Practice_Projects/blob/main/Cat_vs_Dog_Classifier_02_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "currently a work in progress"
      ],
      "metadata": {
        "id": "3SBaY3ZqjEw_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lToBMf0JsUXY"
      },
      "source": [
        "# First Set up the environment and get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsBY_x9TqeyG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQvtJYipaQL"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctf1w3ZvpqSc"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBtgITCPqdKG"
      },
      "outputs": [],
      "source": [
        "#device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNnqXKiFqFtJ"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BviF-nLOrH5q"
      },
      "outputs": [],
      "source": [
        "#walk through the downloaded directory\n",
        "import os\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NYW4AZusLL2"
      },
      "outputs": [],
      "source": [
        "image_path = \"microsoft-catsvsdogs-dataset\"\n",
        "walk_through_dir(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh_wARrNsdBR"
      },
      "source": [
        "# Visualize the data randomly to ensure it was dowloaded successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx0aWr74sYls"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Set seed\n",
        "#random.seed(42)\n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "image_path_list= glob.glob(f\"{image_path}/*/*/*.jpg\")\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = Path(random_image_path).parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC82y8p3vZhq"
      },
      "outputs": [],
      "source": [
        "#visualize with matplot lib\n",
        "\n",
        "#Visualizing the Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.imshow(img)\n",
        "\n",
        "# Optional: Add a title\n",
        "plt.title(f\"Image class: {image_class}\")\n",
        "\n",
        "# Optional: Remove axes ticks for a cleaner image display\n",
        "plt.axis('off')\n",
        "\n",
        "# 3. Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVtp3iABvePe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.axis(False)\n",
        "plt.title(f\"image class: {image_class}, image shape{img_as_array.shape}\") #this would be in the format height, width, color\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpa27tI2vh_v"
      },
      "outputs": [],
      "source": [
        "img_as_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZhO2pSQsmPp"
      },
      "source": [
        "# Now we need to split the data into train, test and validate batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWYNLtuOvusS"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/microsoft-catsvsdogs-dataset/PetImages/Dog\"\n",
        "\n",
        "file_count = 0\n",
        "for root, _, files in os.walk(folder_path):\n",
        "    file_count += len(files)\n",
        "\n",
        "print(f\"Total number of files (including subdirectories) in '{folder_path}': {file_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT6dA6kTvx30"
      },
      "outputs": [],
      "source": [
        "base_dir = \"dataset_split\"\n",
        "categories = [\"Cat\", \"Dog\"]\n",
        "splits = [\"train\", \"val\", \"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE6dkppHSEic"
      },
      "outputs": [],
      "source": [
        "for split in splits:\n",
        "    for category in categories:\n",
        "        folder_path = os.path.join(base_dir, split, category)\n",
        "        os.makedirs(folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi7PmyWGyU6K"
      },
      "outputs": [],
      "source": [
        "#This makes different folders where I will put the test, train and validation folders\n",
        "counts = {split: [] for split in splits}\n",
        "\n",
        "for split in splits:\n",
        "    for category in categories:\n",
        "        folder = os.path.join(base_dir, split, category)\n",
        "        counts[split].append(len(os.listdir(folder)))\n",
        "\n",
        "print(\"Counts (train/val/test) for Cat and Dog:\", counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ8T5V5YzgBL"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "split_ratio = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}\n",
        "#70% to teain, 15 to validation and 15 to test\n",
        "\n",
        "for category in categories:\n",
        "    src_folder = f\"/content/microsoft-catsvsdogs-dataset/PetImages/{category}\"\n",
        "    images = os.listdir(src_folder)\n",
        "    random.shuffle(images)  # shuffle for randomness\n",
        "    total = len(images)\n",
        "\n",
        "    train_end = int(split_ratio[\"train\"] * total)\n",
        "    val_end = train_end + int(split_ratio[\"val\"] * total)\n",
        "\n",
        "    splits_images = {\n",
        "        \"train\": images[:train_end],\n",
        "        \"val\": images[train_end:val_end],\n",
        "        \"test\": images[val_end:]\n",
        "    }\n",
        "\n",
        "    # Copy images to the new folders\n",
        "    for split_name, split_images in splits_images.items():\n",
        "        for img in split_images:\n",
        "            shutil.copy(os.path.join(src_folder, img),\n",
        "                        os.path.join(base_dir, split_name, category, img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL0-mN-L0zNE"
      },
      "outputs": [],
      "source": [
        "#Visualize the total in each group\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69q6ADSZ16dI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.arange(len(categories))  # Cat, Dog\n",
        "width = 0.20  # width of bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x - width, counts[\"train\"], width, label='Train')\n",
        "ax.bar(x, counts[\"val\"], width, label='Validation')\n",
        "ax.bar(x + width, counts[\"test\"], width, label='Test')\n",
        "\n",
        "ax.set_ylabel('Number of Images')\n",
        "ax.set_title('Dataset split counts')\n",
        "ax.set_xticks(x) # Set the tick locations explicitly\n",
        "ax.set_xticklabels(categories)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eddZD3zZ59yi"
      },
      "source": [
        "# Now, we need create data transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpHR93ScO3fi"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def verify_and_clean_dataset(root_folder):\n",
        "    \"\"\"\n",
        "    Walk through all image files and remove any that can't be opened.\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(root_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()  # verify that it is, in fact, an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Removing bad file: {file_path} ({e})\")\n",
        "                os.remove(file_path)\n",
        "\n",
        "# Run it once on your dataset folders:\n",
        "verify_and_clean_dataset(\"dataset_split/train\")\n",
        "verify_and_clean_dataset(\"dataset_split/val\")\n",
        "verify_and_clean_dataset(\"dataset_split/test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1cqbaLLO-XK"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import warnings\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Clean dataset\n",
        "verify_and_clean_dataset(\"dataset_split/train\")\n",
        "verify_and_clean_dataset(\"dataset_split/val\")\n",
        "verify_and_clean_dataset(\"dataset_split/test\")\n",
        "\n",
        "# 2. Define transforms\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),   #resize all images to 128, 128\n",
        "    transforms.ToTensor(), #convert image to tensor data\n",
        "    transforms.Normalize([0.5], [0.5],) # Normalize pixel values\n",
        "])\n",
        "\n",
        "# 3. Create datasets\n",
        "train_dataset = datasets.ImageFolder(\"dataset_split/train\", transform=data_transform)\n",
        "val_dataset = datasets.ImageFolder(\"dataset_split/val\", transform=data_transform)\n",
        "\n",
        "# 4. Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "\n",
        "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"PIL.TiffImagePlugin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbzz-zU663Oq"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),   #resize all images to 128, 128\n",
        "    transforms.ToTensor(), #convert image to tensor data\n",
        "    transforms.Normalize([0.5], [0.5],) # Normalize pixel values\n",
        "])\n",
        "\n",
        "def safe_loader(path):\n",
        "    from PIL import Image\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "    except:\n",
        "        print(f\"Skipping bad image: {path}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S08kMcvV7dXo"
      },
      "outputs": [],
      "source": [
        "#now to load the dataset\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "def safe_loader(path):\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')  # force RGB\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping bad image: {path} (error: {e})\")\n",
        "        return None\n",
        "\n",
        "train_dataset = ImageFolder(root=\"dataset_split/train\", transform=data_transform, loader=safe_loader)\n",
        "val_dataset   = ImageFolder(root=\"dataset_split/val\", transform=data_transform, loader=safe_loader)\n",
        "test_dataset  = ImageFolder(root=\"dataset_split/test\", transform=data_transform, loader=safe_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxz9sXMH8IC2"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYY9I3qt8MTw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b[0] is not None]  # keep only valid (img, label) pairs\n",
        "    if len(batch) == 0:   # if a whole batch is empty, skip it\n",
        "        return torch.Tensor()\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT3y6Bea8gS-"
      },
      "source": [
        "# Define simple CNN for this classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQfmvRYo8k2Z"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CatDogCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CatDogCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # First convolutional layer\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)               # Downsampling\n",
        "        # Adjust the input size of the linear layer based on the output size of the last pooling layer\n",
        "        self.fc1 = nn.Linear(32 * 32 * 32, 1)         # Fully connected layer for binary output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> Pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 -> ReLU -> Pool\n",
        "        x = x.view(x.size(0), -1)          # Flatten before FC, using x.size(0) for batch size\n",
        "        return torch.sigmoid(self.fc1(x))     # Sigmoid for binary output\n",
        "\n",
        "model = CatDogCNN()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7Z6kOql8o49"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du7ceqN2MWZ0"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def check_images(folder):\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for f in files:\n",
        "            path = os.path.join(root, f)\n",
        "            try:\n",
        "                img = Image.open(path)\n",
        "                img.verify()  # verify that it's an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(\"Corrupt image:\", path)\n",
        "\n",
        "check_images(\"dataset_split\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHbDTRwMdIz"
      },
      "source": [
        "An error i ran into in this case is corrupted images. From research it is very common for cat vs dog databases to have corrpted images. So i need to check for it in the imported database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AUHdD70MujH"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_dataset = ImageFolder(\"dataset_split/train\", transform=data_transform,\n",
        "                            is_valid_file=lambda x: True)  # optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9Dn44uIKxAn"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = (correct / total) * 100\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Acc: {train_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qaGeDAJqjCh2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPfUxxJvPhuRUZXrtq6dU3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}